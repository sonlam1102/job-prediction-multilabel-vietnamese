{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Job prediction Bi-GRU-LSTM-CNN singled task description.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UeWV0dAWQ-Jj",
        "0jgqaveDRSml",
        "D439U1_APq9p"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeWV0dAWQ-Jj"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeJhLnZfTRAW",
        "outputId": "27afd747-6301-4934-efa6-e536e848a403"
      },
      "source": [
        "pip install pyvi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.22.2.post1)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.62.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUFUuUnOQ4Go",
        "outputId": "8b492203-6a4b-4101-885e-d32575108d41"
      },
      "source": [
        "pip install tensorflow-gpu==1.15.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.15.0 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.41.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.13.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.37.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (4.8.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xnmdgl4pQ788",
        "outputId": "122f04a1-0505-462c-a194-a40bb7b767c4"
      },
      "source": [
        "pip install keras==2.2.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.4) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHJFQpbXQ9su",
        "outputId": "cef5fc2e-b737-4225-bde3-f3ecd744833d"
      },
      "source": [
        "pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-rx1mpx6s\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-rx1mpx6s\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuCQ0KtIyyB7",
        "outputId": "cff3c436-ccca-43f1-c41a-c6192a71a103"
      },
      "source": [
        "pip install vncorenlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vncorenlp in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Piq2JK2Ryvoq",
        "outputId": "9c465fb0-4a15-41f4-dc8d-fd6e8448fede"
      },
      "source": [
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-15 04:08:09--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   165MB/s    in 0.2s    \n",
            "\n",
            "2021-11-15 04:08:09 (165 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2021-11-15 04:08:09--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-11-15 04:08:09 (13.4 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2021-11-15 04:08:09--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-11-15 04:08:10 (5.71 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DQSDZWtRlz0"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVvxiIAoRkjG"
      },
      "source": [
        "EPOCH = 10\n",
        "MAX_LEN = 80\n",
        "BATCH_SIZE = 64\n",
        "MAX_FEATURE = 10000\n",
        "EMBEDING_DIM = 300\n",
        "NUM_LABEL = 68\n",
        "TOKENIZER = 'vncorenlp'\n",
        "\n",
        "EMBEDDING = 'drive/My Drive/CODE/JobPrediction/embedding/word2vec_vi_words_300dims.txt'\n",
        "\n",
        "TRAIN = 'drive/MyDrive/CODE/JobPrediction/dataset/raw_data_new/train.csv'\n",
        "DEV = 'drive/MyDrive/CODE/JobPrediction/dataset/raw_data_new/dev.csv'\n",
        "TEST = 'drive/MyDrive/CODE/JobPrediction/dataset/raw_data_new/test.csv'\n",
        "\n",
        "LABEL = 'drive/MyDrive/CODE/JobPrediction/dataset/raw_data_new/labels.csv'\n",
        "\n",
        "MODEL_PATH = 'drive/My Drive/CODE/JobPrediction/model_new/single/PhoW2V_300.h5'\n",
        "\n",
        "TASK = 'job_description'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvjXfxwLA0Mu",
        "outputId": "358b0754-2d9d-43c9-e2f7-b2de41fe6d7a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jgqaveDRSml"
      },
      "source": [
        "# Evalutaion metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuAGw0XARQ7O"
      },
      "source": [
        "# Evaluation metric\n",
        "import sys\n",
        "import os\n",
        "import os.path\n",
        "from scipy.stats import sem\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import tensorflow as tf\n",
        "\n",
        "def em_score(y_true, y_pred):\n",
        "    MR = np.all(y_pred == y_true, axis=1).mean()\n",
        "    return MR\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    temp = 0\n",
        "    for i in range(0, len(y_true)):\n",
        "        temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
        "    return temp / len(y_true)\n",
        "\n",
        " \n",
        "def f1_score(y_true, y_pred):\n",
        "    temp = 0\n",
        "    for i in range(len(y_true)):\n",
        "        if (sum(y_true[i]) == 0) and (sum(y_pred[i]) == 0):\n",
        "            continue\n",
        "        temp+= (2*sum(np.logical_and(y_true[i], y_pred[i]))) / (sum(y_true[i])+sum(y_pred[i]))\n",
        "    return temp/ len(y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1wGUpmZQ2xZ"
      },
      "source": [
        "# Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53xHpIlBDJy7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(TRAIN)\n",
        "dev = pd.read_csv(DEV)\n",
        "test = pd.read_csv(TEST)\n",
        "\n",
        "label = pd.read_csv(LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqm2MsZWEmiM"
      },
      "source": [
        "job_types = label['job_type'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH_5FpoXEdEl"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def make_label(data):\n",
        "    lbl_job = []\n",
        "    for td in data['job'].values:\n",
        "        l_job_onehot = np.zeros(len(job_types))\n",
        "        \n",
        "        for i in range(0, len(job_types)):\n",
        "            if job_types[i] in td:\n",
        "                l_job_onehot[i] = 1\n",
        "        lbl_job.append(l_job_onehot)\n",
        "\n",
        "    return lbl_job\n",
        "\n",
        "def return_label(y):\n",
        "    lbl_job = []\n",
        "    for i in range(0, len(y)):\n",
        "        if y[i] == 1:\n",
        "            lbl_job.append(job_types[i])\n",
        "\n",
        "    return lbl_job"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvXx24ltU5eR",
        "outputId": "27c15e3b-3922-4e29-dd85-e01ab99c6cb1"
      },
      "source": [
        "len(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20234"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl_XXf7JU8Us",
        "outputId": "14c86840-93d8-4e7d-a09e-afa44102c035"
      },
      "source": [
        "len(dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1760"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyHQPHdYU93h",
        "outputId": "9f9cd2b7-bbfd-45be-bd00-fded7f03fc13"
      },
      "source": [
        "len(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3933"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D439U1_APq9p"
      },
      "source": [
        "# Word embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFrinuVTPqgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be23418-2b5b-4f42-e614-258f37468ec6"
      },
      "source": [
        "# Read embedding\n",
        "word_dict = []\n",
        "embeddings_index = {}\n",
        "embedding_dim = EMBEDING_DIM\n",
        "max_feature = MAX_FEATURE\n",
        "\n",
        "f = open(EMBEDDING)\n",
        "for line in f:\n",
        "    values = line.split(' ')\n",
        "    word = values[0] \n",
        "    word_dict.append(word)\n",
        "    try:\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    except Exception as e:\n",
        "        pass\n",
        "f.close()\n",
        "\n",
        "print('Embedding data loaded')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding data loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TixA_fQHPvB3"
      },
      "source": [
        "words = word_dict\n",
        "num_words = len(words)\n",
        "\n",
        "# Dictionary word:index pair\n",
        "# word is key and its value is corresponding index\n",
        "word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
        "word_to_index[\"UNK\"] = 1\n",
        "word_to_index[\"PAD\"] = 0\n",
        "\n",
        "# Dictionary lable:index pair\n",
        "idx2word = {i: w for w, i in word_to_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVr7KYRbPzLf"
      },
      "source": [
        "# first create a matrix of zeros, this is our embedding matrix\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "# for each word in out tokenizer lets try to find that work in our w2v model\n",
        "for word, i in word_to_index.items():\n",
        "    if i > max_feature:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # we found the word - add that words vector to the matrix\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # doesn't exist, assign a random vector\n",
        "        embedding_matrix[i] = np.random.randn(embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaIcgCakTji1"
      },
      "source": [
        "# Pre-process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6fwHokpTlTB"
      },
      "source": [
        "y_train = make_label(train)\n",
        "y_dev = make_label(dev)\n",
        "y_test = make_label(test)\n",
        "\n",
        "X_train = train[TASK]\n",
        "X_dev = dev[TASK]\n",
        "X_test = test[TASK]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFQ8ioOMTIwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc86d85-73a3-4dcf-b2fc-97297b5df9a9"
      },
      "source": [
        "from pyvi import ViTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from vncorenlp import VnCoreNLP\n",
        "import re\n",
        "\n",
        "vncorenlp = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "\n",
        "max_len = MAX_LEN\n",
        "\n",
        "def custom_tokenizer(text_data, tokenizer='pyvi'):\n",
        "    # text_data = text_data.lower()\n",
        "    if tokenizer == 'vncorenlp':\n",
        "        # return \" \".join(vncorenlp.tokenize(str(text_data))[0])\n",
        "        text = \"\"\n",
        "        lst = vncorenlp.tokenize(str(text_data))\n",
        "        for t in lst:\n",
        "            text += \" \".join(t)\n",
        "        return text\n",
        "    if tokenizer == 'none':\n",
        "        return text_data\n",
        "    return ViTokenizer.tokenize(str(text_data))\n",
        "\n",
        "def encoding(X, y, tokenizer = True):\n",
        "    sentences = []\n",
        "    \n",
        "    for t in X:\n",
        "        t = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|!?,]\", \"\", t)\n",
        "        sentences.append(custom_tokenizer(t, tokenizer=TOKENIZER))\n",
        "    \n",
        "    # X = []\n",
        "    # for s in sentences:\n",
        "    #     sent = []\n",
        "    #     for w in s.split():\n",
        "    #         try:\n",
        "    #             w = w.lower()\n",
        "    #             sent.append(word_to_index[w])\n",
        "    #         except:\n",
        "    #             sent.append(word_to_index[\"UNK\"])\n",
        "    #     X.append(sent)\n",
        "    \n",
        "    X = tokenizer.texts_to_sequences(sentences)\n",
        "    X = pad_sequences(X, maxlen=max_len)\n",
        "\n",
        "    # X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n",
        "\n",
        "    return (X,y)\n",
        "\n",
        "\n",
        "def decoding(text_data, encoding_text, prediction):\n",
        "    test = [[idx2word[i] for i in row] for row in encoding_text]\n",
        "\n",
        "    lst_token = []\n",
        "\n",
        "    for t in range(0, len(test)):\n",
        "        yy_pred = []\n",
        "        for i in range(0, len(test[t])):\n",
        "            if prediction[t][i] == 1:\n",
        "                yy_pred.append(test[t][i])\n",
        "        lst_token.append(yy_pred)\n",
        "\n",
        "    lis_idx = []\n",
        "    for i in range(0, len(text_data)):\n",
        "        idx = []\n",
        "        for t in lst_token[i]:\n",
        "            index = text_data[i].find(t)\n",
        "            idx.append(index)\n",
        "            for j in range(1, len(t)):\n",
        "                index = index + 1\n",
        "                idx.append(index)\n",
        "        lis_idx.append(idx)\n",
        "\n",
        "    return lis_idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvi9H5ZGXzcW"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(lower=False, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gbYuVVzTnr2"
      },
      "source": [
        "X1, y1 = encoding(X_train, y_train, tokenizer)\n",
        "X2, y2 = encoding(X_dev, y_dev, tokenizer)\n",
        "X3, y3 = encoding(X_test, y_test, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIo6LgSUPtv5"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBoBTK3hOx3v",
        "outputId": "de99f85a-d6a4-404b-86e3-98d2c4538bef"
      },
      "source": [
        "from keras.layers import LSTM, Dense, Concatenate, Embedding, Bidirectional, GlobalMaxPooling1D, Dropout, Reshape, GRU, SpatialDropout1D, Conv1D, GlobalAveragePooling1D\n",
        "from keras.models import Model, Input\n",
        "from keras.utils import plot_model\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "units = 100\n",
        "\n",
        "input = Input(shape = (max_len,))\n",
        "emb = Embedding(input_dim=num_words+2,\n",
        "                output_dim=embedding_dim,\n",
        "                embeddings_initializer=Constant(embedding_matrix),\n",
        "                input_length=max_len,\n",
        "                trainable=True)(input)\n",
        "x1 = SpatialDropout1D(0.2)(emb)\n",
        "\n",
        "x = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
        "x = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
        "    \n",
        "y = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
        "y = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
        "    \n",
        "avg_pool1 = GlobalAveragePooling1D()(x)\n",
        "max_pool1 = GlobalMaxPooling1D()(x)\n",
        "    \n",
        "avg_pool2 = GlobalAveragePooling1D()(y)\n",
        "max_pool2 = GlobalMaxPooling1D()(y)\n",
        "    \n",
        "    \n",
        "x = Concatenate(axis=-1)([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
        "x = Dropout(0.5)(x)\n",
        "out = Dense(68, activation = \"sigmoid\")(x)\n",
        "\n",
        "model = Model(input, out)\n",
        "\n",
        "model.compile(optimizer='adam', loss=BinaryCrossentropy(from_logits=False), metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "plot_model(model ,show_shapes=True,show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 200)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 200, 300)     476253000   input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1 (SpatialDro (None, 200, 300)     0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 200, 200)     240600      spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 200, 200)     320800      spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 199, 50)      20050       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 199, 50)      20050       bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 50)           0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 50)           0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 50)           0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 50)           0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 200)          0           global_average_pooling1d_1[0][0] \n",
            "                                                                 global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_average_pooling1d_2[0][0] \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 200)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 68)           13668       dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 476,868,168\n",
            "Trainable params: 476,868,168\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d5GcLmSS6mW",
        "outputId": "f1940f2a-ad35-4975-a5f3-3ff8b9cac4c9"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "callback = EarlyStopping(monitor='val_loss', patience=1)\n",
        "\n",
        "model.fit(X1, np.array(y1), validation_data=(X2, np.array(y2)), batch_size=256, epochs=30)\n",
        "model.save(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 20234 samples, validate on 1760 samples\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGkImg0ySVs6"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsuobrJZ5uan"
      },
      "source": [
        "y_dev_pred = model.predict(X2)\n",
        "y_test_pred = model.predict(X3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpSAfNkE5-o0"
      },
      "source": [
        "y_dev_pred_new = []\n",
        "\n",
        "for y in y_dev_pred:\n",
        "    lb = []\n",
        "    for i in range(0, len(y)):\n",
        "        if y[i] >= 0.5:\n",
        "            lb.append(1)\n",
        "        else:\n",
        "            lb.append(0)\n",
        "    y_dev_pred_new.append(lb)\n",
        "\n",
        "y_test_pred_new = []\n",
        "\n",
        "for y in y_test_pred:\n",
        "    lb = []\n",
        "    for i in range(0, len(y)):\n",
        "        if y[i] >= 0.5:\n",
        "            lb.append(1)\n",
        "        else:\n",
        "            lb.append(0)\n",
        "    y_test_pred_new.append(lb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvSXD_umNU2y"
      },
      "source": [
        "f1_score(y_dev, y_dev_pred_new)*100, accuracy_score(y_dev, y_dev_pred_new)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C-4xXpYDsDL"
      },
      "source": [
        "f1_score(y_test, y_test_pred_new)*100, accuracy_score(y_test, y_test_pred_new)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMtp8-uv77cp"
      },
      "source": [
        "# Error analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecv0OP-7MuCX"
      },
      "source": [
        "label_dev = []\n",
        "label_dev_pred = []\n",
        "for i in range(len(y_dev)):\n",
        "    label_dev.append(return_label(y_dev[i]))\n",
        "    label_dev_pred.append(return_label(y_dev_pred_new[i]))\n",
        "\n",
        "label_test = []\n",
        "label_test_pred = []\n",
        "for i in range(len(y_test)):\n",
        "    label_test.append(return_label(y_test[i]))\n",
        "    label_test_pred.append(return_label(y_test_pred_new[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBAAnb_TOBUP"
      },
      "source": [
        "dev_result = pd.DataFrame([list(X_dev), label_dev, label_dev_pred]).T\n",
        "test_result = pd.DataFrame([list(X_test), label_test, label_test_pred]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUxC98vAO3Wv"
      },
      "source": [
        "header = ['description', 'true_label', 'predicted_label']\n",
        "dev_result.columns = header\n",
        "test_result.columns = header\n",
        "\n",
        "dev_result.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}